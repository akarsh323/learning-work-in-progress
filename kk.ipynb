import tensorflow as tf
import math

class PositionalEncoding(tf.keras.layers.Layer):
    def __init__(self, emb_size, maxlen=5000, dropout=0.1):
        super(PositionalEncoding, self).__init__()
        self.dropout = tf.keras.layers.Dropout(dropout)
        
        # Create a learnable positional encoding parameter
        # Shape: (maxlen, emb_size)
        self.pos_embedding = tf.Variable(
            initial_value=tf.zeros_initializer()(shape=(maxlen, emb_size)),
            trainable=True
        )
        
        # Initialize
        initializer = tf.initializers.VarianceScaling(scale=2.0, mode='fan_in', distribution='uniform')
        self.pos_embedding.assign(initializer(shape=(maxlen, emb_size)))

    def call(self, inputs):
        # Apply the positional encodings to the input token embeddings
        seq_len = tf.shape(inputs)[1]
        
        # Get the relevant portion of the positional encoding
        pe = self.pos_embedding[:seq_len, :]
        
        # Add positional encoding to each batch
        inputs += tf.expand_dims(pe, axis=0)
        
        return self.dropout(inputs)

# Example usage:
emb_size = 32  # Example embedding size
max_len = 100  # Example maximum sequence length
dropout_rate = 0.1   # Dropout rate

# Initialize positional encoding layer
pos_encoder = PositionalEncoding(emb_size, maxlen=max_len, dropout=dropout_rate)

# Generate example token embeddings (batch size, sequence length, embedding size)
token_embedding = tf.random.normal((1, max_len, emb_size))

# Apply positional encoding
output = pos_encoder(token_embedding)
print(output.shape)
